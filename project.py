# -*- coding: utf-8 -*-
"""PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qR2KRiAHGRBrylYWc8G7nUL5Zm2D2KVu
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

try:
  import pypdf
except ImportError as e:
  raise ImportError("Missing dependency 'pypdf'. Install with 'pip install pypdf' or add it to requirements.txt before deploying the Streamlit app.") from e

documents=[]
reader=pypdf.PdfReader("/content/RIL-Integrated-Annual-Report-2024-25.pdf")
text=""
for page in reader.pages:
  text+=page.extract_text() or ''
documents.append(text)

import nltk
import re
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')

def preprocess_text(text):
  text=text.lower()
  text=re.sub(r'[^a-zA-Z0-9\s.]','',text)
  text=re.sub(r'\n','',text)
  return text

doc_txt=[preprocess_text(doc) for doc in documents]

from nltk.tokenize import sent_tokenize,word_tokenize

sent=sent_tokenize(str(doc_txt))

from textblob import TextBlob
def analyze_sentiment(text):
    analysis=TextBlob(text)
    if analysis.sentiment.polarity>0:
        return "Positive"
    elif analysis.sentiment.polarity==0:
        return "Neutral"
    else:
        return "Negative"

sent=pd.DataFrame(sent,columns=['text'])

sent['sentiment']=[str(analyze_sentiment(x)) for x in sent['text']]

sent['sentiment'].value_counts()

sent.head()

try:
  import tabula
except ImportError as e:
  raise ImportError("Missing dependency 'tabula-py'. Install with 'pip install tabula-py' and ensure Java is installed on the host.") from e

tables=tabula.read_pdf("/content/RIL-Integrated-Annual-Report-2024-25.pdf",
                       pages="all")

for i,df in enumerate(tables):
  print(f"Table {i+1}:\n{df}\n")

df = tables[124]
df

msme_words=word_tokenize(str(doc_txt))

len(msme_words)

from nltk.corpus import stopwords
en_stopwords=set(stopwords.words('english'))
msme_words=[w for w in msme_words if not w in en_stopwords]

msme_words=[w for w in msme_words if len(w)>2]

from nltk.probability import FreqDist

freq_words=FreqDist(msme_words)

freq_words.most_common(20)

from wordcloud import WordCloud

wordcloud=WordCloud(width=1000,height=500).generate(str(msme_words))
plt.imshow(wordcloud)

import numpy as np
from sklearn.feature_extraction.text import CountVectorizer

DTM=CountVectorizer(max_features=30,stop_words="english")
# max_features = number of columns/words to be considered (Top N)

X_DTM=DTM.fit_transform(sent['text'])

pd.DataFrame(X_DTM.toarray(),columns=DTM.get_feature_names_out()).head()

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf=TfidfVectorizer(max_features=300,stop_words="english")

X_tfidf=tfidf.fit_transform(sent['text'])

pd.DataFrame(X_tfidf.toarray(),columns=tfidf.get_feature_names_out()).head()

tfidf_bigrams=TfidfVectorizer(max_features=300,stop_words="english",ngram_range=(2,2))

X_tfidf_bigrams=tfidf_bigrams.fit_transform(sent['text'])

pd.DataFrame(X_tfidf_bigrams.toarray(),
             columns=tfidf_bigrams.get_feature_names_out()).head()

