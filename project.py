# -*- coding: utf-8 -*-
"""PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qR2KRiAHGRBrylYWc8G7nUL5Zm2D2KVu
"""

try:
  import pandas as pd
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'pandas'. Add 'pandas' to requirements.txt and redeploy, or run `pip install pandas` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'pandas'. Install with 'pip install pandas' or add it to requirements.txt before deploying the Streamlit app.")

try:
  import numpy as np
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'numpy'. Add 'numpy' to requirements.txt and redeploy, or run `pip install numpy` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'numpy'. Install with 'pip install numpy' or add it to requirements.txt before deploying the Streamlit app.")
try:
  import matplotlib.pyplot as plt
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'matplotlib'. Add 'matplotlib' to requirements.txt and redeploy, or run `pip install matplotlib` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'matplotlib'. Install with 'pip install matplotlib' or add it to requirements.txt before deploying the Streamlit app.")

try:
  import pypdf
except ImportError as e:
  raise ImportError("Missing dependency 'pypdf'. Install with 'pip install pypdf' or add it to requirements.txt before deploying the Streamlit app.") from e

documents=[]
import streamlit as st
import pypdf

uploaded_file = st.file_uploader("Upload a PDF", type=["pdf"])

if uploaded_file is None:
    st.warning("Please upload a PDF file to continue.")
    st.stop()

reader = pypdf.PdfReader(uploaded_file)

text = ""
for page in reader.pages:
    text += page.extract_text() or ""



try:
  import nltk
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'nltk'. Add 'nltk' to requirements.txt and redeploy, or run `pip install nltk` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'nltk'. Install with 'pip install nltk' or add it to requirements.txt before deploying the Streamlit app.")

import re
try:
  nltk.download('stopwords')
  nltk.download('punkt')
  nltk.download('punkt_tab')
except Exception:
  pass

def preprocess_text(text):
  text=text.lower()
  text=re.sub(r'[^a-zA-Z0-9\s.]','',text)
  text=re.sub(r'\n','',text)
  return text

doc_txt=[preprocess_text(doc) for doc in documents]

from nltk.tokenize import sent_tokenize,word_tokenize

sent=sent_tokenize(str(doc_txt))

try:
  from textblob import TextBlob
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'textblob'. Add 'textblob' to requirements.txt and redeploy, or run `pip install textblob` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'textblob'. Install with 'pip install textblob' or add it to requirements.txt before deploying the Streamlit app.")
def analyze_sentiment(text):
    analysis=TextBlob(text)
    if analysis.sentiment.polarity>0:
        return "Positive"
    elif analysis.sentiment.polarity==0:
        return "Neutral"
    else:
        return "Negative"

sent=pd.DataFrame(sent,columns=['text'])

sent['sentiment']=[str(analyze_sentiment(x)) for x in sent['text']]

sent['sentiment'].value_counts()

sent.head()



import tempfile

st.subheader("üìä Extract Tables from PDF")

# Save the uploaded PDF temporarily so tabula can read it
with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
    tmp_file.write(uploaded_file.read())
    temp_pdf_path = tmp_file.name

# Use the extracted PDF text
clean_text = preprocess_text(text)

# Tokenize
msme_words = word_tokenize(clean_text)

# Remove stopwords
from nltk.corpus import stopwords
stop = set(stopwords.words("english"))
msme_words = [w for w in msme_words if w not in stop and len(w) > 2]

# If no words found
if len(msme_words) == 0:
    st.error("No words found to generate WordCloud. Check PDF content.")
else:
    st.subheader("‚òÅÔ∏è Word Cloud")
    wc = WordCloud(width=1000, height=500, background_color="white")
    wc_image = wc.generate(" ".join(msme_words))

    fig = plt.figure(figsize=(10, 5))
    plt.imshow(wc_image, interpolation="bilinear")
    plt.axis("off")
    st.pyplot(fig)


import numpy as np
try:
  from sklearn.feature_extraction.text import CountVectorizer
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'scikit-learn'. Add 'scikit-learn' to requirements.txt and redeploy, or run `pip install scikit-learn` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'scikit-learn'. Install with 'pip install scikit-learn' or add it to requirements.txt before deploying the Streamlit app.")

DTM=CountVectorizer(max_features=30,stop_words="english")
# max_features = number of columns/words to be considered (Top N)

X_DTM=DTM.fit_transform(sent['text'])

pd.DataFrame(X_DTM.toarray(),columns=DTM.get_feature_names_out()).head()

try:
  from sklearn.feature_extraction.text import TfidfVectorizer
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'scikit-learn'. Add 'scikit-learn' to requirements.txt and redeploy, or run `pip install scikit-learn` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'scikit-learn'. Install with 'pip install scikit-learn' or add it to requirements.txt before deploying the Streamlit app.")

tfidf=TfidfVectorizer(max_features=300,stop_words="english")

X_tfidf=tfidf.fit_transform(sent['text'])

pd.DataFrame(X_tfidf.toarray(),columns=tfidf.get_feature_names_out()).head()

tfidf_bigrams=TfidfVectorizer(max_features=300,stop_words="english",ngram_range=(2,2))

X_tfidf_bigrams=tfidf_bigrams.fit_transform(sent['text'])

pd.DataFrame(X_tfidf_bigrams.toarray(),
             columns=tfidf_bigrams.get_feature_names_out()).head()

