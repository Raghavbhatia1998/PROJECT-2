# -*- coding: utf-8 -*-
"""PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qR2KRiAHGRBrylYWc8G7nUL5Zm2D2KVu
"""

try:
  import pandas as pd
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'pandas'. Add 'pandas' to requirements.txt and redeploy, or run `pip install pandas` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'pandas'. Install with 'pip install pandas' or add it to requirements.txt before deploying the Streamlit app.")

try:
  import numpy as np
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'numpy'. Add 'numpy' to requirements.txt and redeploy, or run `pip install numpy` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'numpy'. Install with 'pip install numpy' or add it to requirements.txt before deploying the Streamlit app.")
try:
  import matplotlib.pyplot as plt
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'matplotlib'. Add 'matplotlib' to requirements.txt and redeploy, or run `pip install matplotlib` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'matplotlib'. Install with 'pip install matplotlib' or add it to requirements.txt before deploying the Streamlit app.")

try:
  import pypdf
except ImportError as e:
  raise ImportError("Missing dependency 'pypdf'. Install with 'pip install pypdf' or add it to requirements.txt before deploying the Streamlit app.") from e

documents=[]
uploaded_file = st.file_uploader("Upload a PDF", type=["pdf"])

if uploaded_file:
    reader = pypdf.PdfReader(uploaded_file)
    text = ""
    for page in reader.pages:
        text += page.extract_text() or ""


try:
  import nltk
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'nltk'. Add 'nltk' to requirements.txt and redeploy, or run `pip install nltk` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'nltk'. Install with 'pip install nltk' or add it to requirements.txt before deploying the Streamlit app.")

import re
try:
  nltk.download('stopwords')
  nltk.download('punkt')
  nltk.download('punkt_tab')
except Exception:
  pass

def preprocess_text(text):
  text=text.lower()
  text=re.sub(r'[^a-zA-Z0-9\s.]','',text)
  text=re.sub(r'\n','',text)
  return text

doc_txt=[preprocess_text(doc) for doc in documents]

from nltk.tokenize import sent_tokenize,word_tokenize

sent=sent_tokenize(str(doc_txt))

try:
  from textblob import TextBlob
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'textblob'. Add 'textblob' to requirements.txt and redeploy, or run `pip install textblob` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'textblob'. Install with 'pip install textblob' or add it to requirements.txt before deploying the Streamlit app.")
def analyze_sentiment(text):
    analysis=TextBlob(text)
    if analysis.sentiment.polarity>0:
        return "Positive"
    elif analysis.sentiment.polarity==0:
        return "Neutral"
    else:
        return "Negative"

sent=pd.DataFrame(sent,columns=['text'])

sent['sentiment']=[str(analyze_sentiment(x)) for x in sent['text']]

sent['sentiment'].value_counts()

sent.head()

try:
  import tabula
except ImportError as e:
  raise ImportError("Missing dependency 'tabula-py'. Install with 'pip install tabula-py' and ensure Java is installed on the host.") from e

tables=tabula.read_pdf("/content/RIL-Integrated-Annual-Report-2024-25.pdf",
                       pages="all")

for i,df in enumerate(tables):
  print(f"Table {i+1}:\n{df}\n")

df = tables[124]
df

msme_words=word_tokenize(str(doc_txt))

len(msme_words)

from nltk.corpus import stopwords
en_stopwords=set(stopwords.words('english'))
msme_words=[w for w in msme_words if not w in en_stopwords]

msme_words=[w for w in msme_words if len(w)>2]

from nltk.probability import FreqDist

freq_words=FreqDist(msme_words)

freq_words.most_common(20)

try:
  from wordcloud import WordCloud
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'wordcloud'. Add 'wordcloud' to requirements.txt and redeploy, or run `pip install wordcloud` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'wordcloud'. Install with 'pip install wordcloud' or add it to requirements.txt before deploying the Streamlit app.")

wordcloud=WordCloud(width=1000,height=500).generate(str(msme_words))
plt.imshow(wordcloud)

import numpy as np
try:
  from sklearn.feature_extraction.text import CountVectorizer
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'scikit-learn'. Add 'scikit-learn' to requirements.txt and redeploy, or run `pip install scikit-learn` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'scikit-learn'. Install with 'pip install scikit-learn' or add it to requirements.txt before deploying the Streamlit app.")

DTM=CountVectorizer(max_features=30,stop_words="english")
# max_features = number of columns/words to be considered (Top N)

X_DTM=DTM.fit_transform(sent['text'])

pd.DataFrame(X_DTM.toarray(),columns=DTM.get_feature_names_out()).head()

try:
  from sklearn.feature_extraction.text import TfidfVectorizer
except ImportError:
  try:
    import streamlit as st
    st.error("Missing dependency 'scikit-learn'. Add 'scikit-learn' to requirements.txt and redeploy, or run `pip install scikit-learn` locally.")
    st.stop()
  except Exception:
    raise ImportError("Missing dependency 'scikit-learn'. Install with 'pip install scikit-learn' or add it to requirements.txt before deploying the Streamlit app.")

tfidf=TfidfVectorizer(max_features=300,stop_words="english")

X_tfidf=tfidf.fit_transform(sent['text'])

pd.DataFrame(X_tfidf.toarray(),columns=tfidf.get_feature_names_out()).head()

tfidf_bigrams=TfidfVectorizer(max_features=300,stop_words="english",ngram_range=(2,2))

X_tfidf_bigrams=tfidf_bigrams.fit_transform(sent['text'])

pd.DataFrame(X_tfidf_bigrams.toarray(),
             columns=tfidf_bigrams.get_feature_names_out()).head()

